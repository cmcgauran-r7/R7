{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Complete Project Workflow in Amazon SageMaker\n",
    "### Model Deployment\n",
    "    \n",
    "1. [Local Mode endpoint](#LocalModeEndpoint)\n",
    "2. [SageMaker hosted endpoint](#SageMakerHostedEndpoint)\n",
    "3. [Multi-Model endpoints](#MultiModelEndpoints)\n",
    "4. [Production Variants with Model Monitor](#ProductionVariants)\n",
    "5. [Invoking SageMaker endpoints](#InvokingSageMakerEndpoints)\n",
    "6. [Clean up resources](#CleanUp)\n",
    "\n",
    "## Local Mode endpoint <a class=\"anchor\" id=\"LocalModeEndpoint\">\n",
    "\n",
    "While Amazon SageMakerâ€™s Local Mode training is very useful to make sure your training code is working before moving on to full scale training, it also would be useful to have a convenient way to test your model locally before incurring the time and expense of deploying it to production. One possibility is to fetch the PyTorch artifact or a model checkpoint saved in Amazon S3, and load it in your notebook for testing. However, an even easier way to do this is to use the SageMaker Python SDK to do this work for you by setting up a Local Mode endpoint.\n",
    "\n",
    "More specifically, the Estimator object from the Local Mode training job can be used to deploy a model locally. With one exception, this code is the same as the code you would use to deploy to production. In particular, all you need to do is invoke the local Estimator's deploy method, and similarly to Local Mode training, specify the instance type as either `local_gpu` or `local` depending on whether your notebook is on a GPU instance or CPU instance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the variables stored from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following single line of code deploys the model locally in the SageMaker PyTorch container using the model artifacts from our local training job:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "local_model = PyTorchModel(entry_point='train_deploy.py', source_dir='pytorch-model/train_model',\n",
    "                           model_data=local_model_data, role=role, framework_version='1.5.1')\n",
    "local_predictor = local_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get predictions from the Local Mode endpoint, simply invoke the Predictor's predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, json_serializer\n",
    "import json\n",
    "\n",
    "local_predictor.content_type = \"application/json\"\n",
    "local_predictor.accept = \"application/json\"\n",
    "local_predictor.serializer = json_serializer\n",
    "local_predictor.deserializer = json_deserializer\n",
    "\n",
    "result = local_predictor.predict(x_test[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, the predictions can be compared against the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_results = [local_predictor.predict(x_test[i]) for i in range(0, 10)]\n",
    "print(f'predictions: \\t {local_results}')\n",
    "print(f'target values: \\t {y_test[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only trained the model for a few epochs and there is much room for improvement, but the predictions so far should at least appear reasonably within the ballpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having the SageMaker PyTorch Serving container indefinitely running locally, simply gracefully shut it down by calling the `delete_endpoint` method of the Predictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker hosted endpoint <a class=\"anchor\" id=\"SageMakerHostedEndpoint\">\n",
    "\n",
    "Assuming the best model from the tuning job is better than the model produced by the individual Hosted Training job above, we could now easily deploy that model to production.  A convenient option is to use a SageMaker hosted endpoint, which serves real time predictions from the trained model (Batch Transform jobs also are available for asynchronous, offline predictions on large datasets). The endpoint will retrieve the PyTorch SavedModel created during training and deploy it within a SageMaker PyTorch Serving container. This all can be accomplished with one line of code.  \n",
    "\n",
    "More specifically, by calling the `deploy` method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint.  It will take several minutes longer to deploy the model to the hosted endpoint compared to the Local Mode endpoint, which is more useful for fast prototyping of inference code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(entry_point='train_deploy.py', source_dir='pytorch-model/train_model',\n",
    "                     model_data=remote_model_data, role=role, framework_version='1.5.1',\n",
    "                     name='pytorch-model-from-hosted-endpoint')\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium',\n",
    "                         endpoint_name='pytorch-housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get predictions from the hosted endpoint, simply invoke the Predictor's predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, json_serializer\n",
    "import json\n",
    "\n",
    "predictor.content_type = \"application/json\"\n",
    "predictor.accept = \"application/json\"\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "predictor.predict(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the predictions generated by this endpoint with those generated locally by the Local Mode endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosted_results = [predictor.predict(x_test[i]) for i in range(0, 10)]\n",
    "print(f'local predictions: \\t {local_results}')\n",
    "print(f'hosted predictions: \\t {hosted_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker hosted endpoint with autotuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "tuner_parameters['estimator'] = estimator\n",
    "\n",
    "tuner = HyperparameterTuner(**tuner_parameters)\n",
    "tuner = tuner.attach(tuning_job_name)\n",
    "tuning_predictor = tuner.deploy(initial_instance_count=1, instance_type='ml.t2.medium',\n",
    "                                endpoint_name='pytorch-housing-auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the predictions generated by this endpoint with those generated locally by the Local Mode endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_predictor.content_type = \"application/json\"\n",
    "tuning_predictor.accept = \"application/json\"\n",
    "tuning_predictor.serializer = json_serializer\n",
    "tuning_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosted_results = [tuning_predictor.predict(x_test[i]) for i in range(0, 10)]\n",
    "print(f'local predictions: \\t {local_results}')\n",
    "print(f'tuner predictions: \\t {hosted_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Endpoints <a class=\"anchor\" id=\"MultiModelEndpoints\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hundreds or thousands of models deployed to hundreds or thousands of endpoints can get costly. It's even more challenging when you don't have to access all models at the same time but still need them to be available at all times.\n",
    "\n",
    "SageMaker multi-model endpoints allows you to deploy multiple models on a single serving container. This drastically reduces your costs without sacrificing scalability and low latency. Check out more about it [here](https://aws.amazon.com/blogs/machine-learning/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = remote_model_data\n",
    "model_2 = f's3://{bucket}/{tuner.best_training_job()}/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the two PyTorch models to the same prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade awscli==1.18.140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = f's3://{bucket}/{s3_prefix}/mme/model1.tar.gz'\n",
    "output_2 = f's3://{bucket}/{s3_prefix}/mme/model2.tar.gz'\n",
    "\n",
    "!aws s3 cp {model_1} {output_1}\n",
    "!aws s3 cp {model_2} {output_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the Multi-Model Endpoint container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.predictor import json_serializer\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "image = '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.5.1-cpu-py36-ubuntu16.04'\n",
    "\n",
    "# All models are located under this prefix, each with a unique name\n",
    "model_data_prefix = f's3://{bucket}/{s3_prefix}/mme/'\n",
    "\n",
    "mme = MultiDataModel(name='mme-pytorch',\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model,\n",
    "                     sagemaker_session=sess)\n",
    "\n",
    "mme_predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.t2.medium',\n",
    "                       endpoint_name='mme-pytorch')\n",
    "\n",
    "mme_predictor.serializer = json_serializer\n",
    "mme_predictor.content_type = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of models we can choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {model_data_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_predictor.predict(x_test[0], initial_args={'TargetModel': 'model1.tar.gz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_predictor.predict(x_test[0], initial_args={'TargetModel': 'model2.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a model to this endpoint on-the-fly by dropping a `x.targ.gz` package in the `model_data_prefix` location in S3. Just make sure it's a PyTorch model and that it has a unique name from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to copy an existing model and just rename it for demo purposes\n",
    "\n",
    "output_3 = f's3://{bucket}/{s3_prefix}/mme/model3.tar.gz'\n",
    "!aws s3 cp {model_1} {output_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {model_data_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 3rd model is available to call, and we shall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_predictor.predict(x_test[0], initial_args={'TargetModel': 'model3.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Variants with Model Monitor <a class=\"anchor\" id=\"ProductionVariants\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "# Create the first model\n",
    "model = PyTorchModel(entry_point='train_deploy.py', source_dir='pytorch-model/train_model',\n",
    "                     model_data=remote_model_data, role=role, framework_version='1.5.1',\n",
    "                     name='pytorch-model', sagemaker_session=sess)\n",
    "model._create_sagemaker_model(instance_type='ml.t2.medium')\n",
    "\n",
    "# And now the second PyTorch model is already created from the autotuned job,\n",
    "# so we'll just use that `tuner.best_training_job()`\n",
    "\n",
    "# PyTorch model with our own hyperparameters\n",
    "variant_1 = production_variant(model_name='pytorch-model',\n",
    "                               instance_type='ml.t2.medium',\n",
    "                               initial_instance_count=1,\n",
    "                               variant_name='Variant1',\n",
    "                               initial_weight=1)\n",
    "\n",
    "# PyTorch model with autotuned hyperparameters\n",
    "variant_2 = production_variant(model_name=tuner.best_training_job(),\n",
    "                               instance_type='ml.t2.medium',\n",
    "                               initial_instance_count=1,\n",
    "                               variant_name='Variant2',\n",
    "                               initial_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Model Monitor's Data Capture for Production Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "s3_capture_upload_path = f's3://{bucket}/{s3_prefix}/model_monitor'\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture=True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path)\n",
    "\n",
    "data_capture_config_dict = data_capture_config._to_request_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the Production Variant endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'pytorch-production-variants'\n",
    "sess.endpoint_from_production_variants(name=endpoint_name,\n",
    "                                       production_variants=[variant_1, variant_2],\n",
    "                                       data_capture_config_dict=data_capture_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "import numpy as np\n",
    "\n",
    "# Upload our training data to S3 as a CSV\n",
    "local_train_data = np.load('./data/train/x_train.npy')\n",
    "np.savetxt('./data/train.csv', local_train_data, delimiter=',', fmt='%f')\n",
    "out = f's3://{bucket}/{s3_prefix}/data/train.csv'\n",
    "!aws s3 cp ./data/train.csv {out}\n",
    "\n",
    "# Baseline data is the training data that we saved as CSV\n",
    "baseline_data_uri = f's3://{bucket}/{s3_prefix}/data/train.csv'\n",
    "baseline_results_uri = f's3://{bucket}/{s3_prefix}/model_monitor/baseline_output'\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "baseline_violations_uri = f's3://{bucket}/{s3_prefix}/model_monitor/violations'\n",
    "\n",
    "monitor_schedule_name = 'pytorch-boston-housing-model-monitor-schedule'\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=baseline_violations_uri,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the production variant endpoint so that SageMaker Model Monitor can capture some traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "prodvar_predictor = RealTimePredictor(endpoint=endpoint_name,\n",
    "                              sagemaker_session=sess,\n",
    "                              serializer=json_serializer,\n",
    "                              deserializer=json_deserializer,\n",
    "                              content_type='application/json',\n",
    "                              accept='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant1_predictions = []\n",
    "for i in range(len(x_test)):\n",
    "    variant1_predictions.append(prodvar_predictor.predict(x_test[i], target_variant='Variant1'))\n",
    "variant1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant2_predictions = []\n",
    "for i in range(len(x_test)):\n",
    "    variant2_predictions.append(prodvar_predictor.predict(x_test[i], target_variant='Variant2'))\n",
    "variant2_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some time has passed by and you want to grab the predictions that Model Monitor captured for us and compare them to what actually happened. As such, we can detect for model drift and even do an A/B test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant1_predictions = f'{s3_capture_upload_path}/{endpoint_name}/Variant1'\n",
    "!aws s3 cp --recursive {variant1_predictions} ./data/model_monitor/Variant1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk('./data/model_monitor/Variant1'):\n",
    "     for file in files:\n",
    "        if file != '.DS_Store':\n",
    "            with open(os.path.join(root, file), \"r\", encoding = \"utf-8\") as auto:\n",
    "                requests_predictions_file = auto.readlines()\n",
    "\n",
    "variant1_predictions = []\n",
    "for i in range(len(requests_predictions_file)):\n",
    "    variant1_predictions.append(float(json.loads(requests_predictions_file[i])['captureData']['endpointOutput']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant2_predictions = f'{s3_capture_upload_path}/{endpoint_name}/Variant2'\n",
    "!aws s3 cp --recursive {variant2_predictions} ./data/model_monitor/Variant2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('./data/model_monitor/Variant2'):\n",
    "     for file in files:\n",
    "        if file != '.DS_Store':\n",
    "            with open(os.path.join(root, file), \"r\", encoding = \"utf-8\") as auto:\n",
    "                requests_predictions_file = auto.readlines()\n",
    "\n",
    "variant2_predictions = []\n",
    "for i in range(len(requests_predictions_file)):\n",
    "    variant2_predictions.append(float(json.loads(requests_predictions_file[i])['captureData']['endpointOutput']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = y_test.tolist()\n",
    "list(zip(ground_truth, variant1_predictions))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(ground_truth, variant2_predictions))[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Levene's test to assess the equality of variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "errors_var_1 = [ground_truth[i] - variant1_predictions[i] for i in range(len(ground_truth))]\n",
    "errors_var_2 = [ground_truth[i] - variant2_predictions[i] for i in range(len(ground_truth))]\n",
    "\n",
    "stat, p = levene(errors_var_1, errors_var_2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not enough evidence to suggest the errors between both models were unlikely due to random chance.\n",
    "\n",
    "If there was, you'd need to reroute traffic to the better production variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "response = sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'DesiredWeight': 25,\n",
    "            'VariantName': variant_1['VariantName']\n",
    "        },\n",
    "        {\n",
    "            'DesiredWeight': 75,\n",
    "            'VariantName': variant_2['VariantName']\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking SageMaker Endpoints <a class=\"anchor\" id=\"InvokingSageMakerEndpoints\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code so far, we've seen examples of training a model, deploying it as an endpoint, then using that deployed model object to do predictions. But what if we want to call an existing SageMaker endpoint? Well, there are a couple ways to do this. The first is with SageMaker's Python SDK and the second with boto3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling an endpoint with SageMaker's Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "predictor = RealTimePredictor(endpoint='pytorch-housing',\n",
    "                              sagemaker_session=sess,\n",
    "                              serializer=json_serializer,\n",
    "                              deserializer=json_deserializer)\n",
    "\n",
    "predictor.predict(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or call an endpoint using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "# Stringify the numpy array\n",
    "payload = str(x_test[0].tolist())\n",
    "prediction = sm_runtime.invoke_endpoint(EndpointName='pytorch-housing',\n",
    "                                         ContentType='application/json',\n",
    "                                         Body=payload)\n",
    "prediction = json.loads(prediction['Body'].read())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up <a class=\"anchor\" id=\"CleanUp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid billing charges from stray resources, you can delete the prediction endpoint to release its associated instance(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "tuning_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "mme_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "!aws sagemaker delete-monitoring-schedule --monitoring-schedule-name pytorch-boston-housing-model-monitor-schedule\n",
    "# Manually delete production variant endpoint (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
